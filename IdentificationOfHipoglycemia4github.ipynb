{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywno3GqLOUVX"
      },
      "source": [
        "## Load data (from files, generated from a database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELDqu5Qhwkxz"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import re\n",
        "from re import split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rysyEFUTNW-v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') # donde gdrive = Mi unidad en google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoKTnEASwvoW"
      },
      "outputs": [],
      "source": [
        "#route ='C:/Users/diabetes.ml1/Downloads'\n",
        "# route google drive\n",
        "ruta = 'gdrive/My Drive/Colab Notebooks/diabetes_codigoCristhianPatino/Data_RiesgoHipoglicemia'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4nSpkm-wx8s"
      },
      "outputs": [],
      "source": [
        "archivo1 = ruta + \"/\" + 'analisis_hemog_unbalanced.csv'\n",
        "archivo2 = ruta + \"/\" + 'hemog_text_panel_complete.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehOJcJ68wzwu"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(archivo1 ,sep=\",\", low_memory=False)\n",
        "print(df1.shape)\n",
        "df1.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbDFKxvwHGh7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(archivo2 ,sep=\",\", low_memory=False)\n",
        "print(df2.shape)\n",
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colnames\n",
        "df2.columns"
      ],
      "metadata": {
        "id": "UTSowPuuH9wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmfB93hhHTFF"
      },
      "outputs": [],
      "source": [
        "# merge files\n",
        "df3 = pd.merge(df1[['KeyAnonimo', 'fecha_consulta', 'HbA1c', 'analisis']], df2[['KeyAnonimo', 'edad', 'genero']], how = 'inner', on='KeyAnonimo')\n",
        "df3 = df3.drop_duplicates()\n",
        "df3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKgnuv25dIua"
      },
      "outputs": [],
      "source": [
        "df3 = df3.reset_index()\n",
        "df3.drop(['index'], axis = 1, inplace = True)\n",
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "JBDI5PyClYZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pzA7SKbVpNC"
      },
      "outputs": [],
      "source": [
        "df3['fecha_consulta'] = pd.to_datetime(df3['fecha_consulta'], format = '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lps2sHpWX4g"
      },
      "outputs": [],
      "source": [
        "# adjust patient's age\n",
        "df3.loc[df3['fecha_consulta'].isin(pd.date_range('2018-01-01', '2018-12-31')), 'edad'] = df3['edad'] - 3\n",
        "df3.loc[df3['fecha_consulta'].isin(pd.date_range('2019-01-01', '2019-12-31')), 'edad'] = df3['edad'] - 2\n",
        "df3.loc[df3['fecha_consulta'].isin(pd.date_range('2020-01-01', '2020-12-31')), 'edad'] = df3['edad'] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkwdVjfIazsl"
      },
      "outputs": [],
      "source": [
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of unique patients\n",
        "len(pd.unique(df3['KeyAnonimo']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwzPBmb2vjTe",
        "outputId": "b7258ed2-345f-47be-94c0-ac6916841111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23802"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cl: contar por genero\n",
        "patients_grouped = df3.groupby('KeyAnonimo').first()\n",
        "male_count = patients_grouped[patients_grouped.genero == 'M'].genero.count()\n",
        "female_count = patients_grouped[patients_grouped.genero == 'F'].genero.count()"
      ],
      "metadata": {
        "id": "2lL_LVRKs9vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_count"
      ],
      "metadata": {
        "id": "j-qVIBjeu6NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "female_count"
      ],
      "metadata": {
        "id": "u8yxZb0vvIo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze age\n",
        "grouped = df3.groupby('KeyAnonimo')"
      ],
      "metadata": {
        "id": "i5BAUmPVhEas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average age per patient, during the 3 years of observation:\n",
        "averages = grouped.mean() # determines the average for age and HbA1c, as numerical fields...\n",
        "averages"
      ],
      "metadata": {
        "id": "3-ZGImE_h4_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average age of all patients\n",
        "averages.edad.mean()"
      ],
      "metadata": {
        "id": "gfntN5ODijwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# age: standard deviation of all patients\n",
        "averages.edad.std()"
      ],
      "metadata": {
        "id": "_fTR4zzANAWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average age of the patient: assign the calculated average to an age dataframe\n",
        "edades = pd.DataFrame(averages)\n",
        "edades = edades.reset_index().rename(columns={'index': 'KeyAnonimo'})\n",
        "list(edades.columns)"
      ],
      "metadata": {
        "id": "gkqWTl600-ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# age by range (groups)\n",
        "bins = [18, 30, 40, 50, 60, 70, 120]\n",
        "labels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
        "edades['rango_edad'] = pd.cut(edades.edad, bins, labels = labels,include_lowest = True)\n",
        "edades['rango_edad'].cat.add_categories('unknown').fillna('unknown')"
      ],
      "metadata": {
        "id": "Wo8Wls395A7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count by age ranges\n",
        "age_range_counts = edades.groupby('rango_edad')['rango_edad'].count()\n",
        "print(age_range_counts)"
      ],
      "metadata": {
        "id": "9v7ZuTqY6qeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patient's average HbA1c: assign the calculated average to a dataframe\n",
        "hemoglobina = pd.DataFrame(averages)\n",
        "hemoglobina = hemoglobina.reset_index().rename(columns={'index': 'KeyAnonimo'})\n",
        "list(hemoglobina.columns)"
      ],
      "metadata": {
        "id": "YmH9MS3AOVDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descriptive statistics for HbA1c:\n",
        "hemoglobina.HbA1c.describe()"
      ],
      "metadata": {
        "id": "fgJk4NqwRPcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HbA1c by range\n",
        "bins = [4.0, 4.8, 5.7, 6.4, 7.0, 8.0, 17]\n",
        "labels = ['4.0-4.799', '4.8-5.699', '5.7-6.399', '6.4-6.999', '7.0-7.999', '8.0+']\n",
        "hemoglobina['rango_hemoglobina'] = pd.cut(hemoglobina.HbA1c, bins, labels = labels,include_lowest = True)\n",
        "hemoglobina['rango_hemoglobina'].cat.add_categories('unknown').fillna('unknown')"
      ],
      "metadata": {
        "id": "beQzqrXbN9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count by HbA1c ranges / groups\n",
        "hemoglobina_range_counts = hemoglobina.groupby('rango_hemoglobina')['rango_hemoglobina'].count()\n",
        "print(hemoglobina_range_counts)"
      ],
      "metadata": {
        "id": "8BGqQF0zU7_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJjSB-tzBEzw"
      },
      "outputs": [],
      "source": [
        "df3['analisis'] = df3['analisis'].astype(str)\n",
        "#df3 = df3.iloc[:30000,:]\n",
        "#df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ydFUQbK3sb5"
      },
      "outputs": [],
      "source": [
        "# to lower case\n",
        "for i in range(0,df3.shape[0]):\n",
        "  df3['analisis'][i] = df3.analisis[i].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRhYMJq33slD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# basic cleaning for characters\n",
        "quitar = \",;:.+-()/?´*%\"\n",
        "\n",
        "for i in range(0,df3.shape[0]):\n",
        "    for caracter in quitar:\n",
        "       df3.analisis[i] = df3.analisis[i].replace(caracter, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQLa-95Z4GX5"
      },
      "outputs": [],
      "source": [
        "# replace spanish characters\n",
        "quitar    = [\"á\", \"é\", \"í\", \"ó\", \"ú\"]\n",
        "remplazar = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
        "\n",
        "for j in range(0,df3.shape[0]):\n",
        "    for i in range(0,5):\n",
        "        df3.analisis[j] = df3.analisis[j].replace(quitar[i], remplazar[i])\n",
        "\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5DDiBhhJ_k-"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "# df3.to_csv('DF_RiesgoHip.csv', sep=';')\n",
        "# Here, the reading of the already preprocessed data begins (from gdrive)\n",
        "archivo_final = ruta + \"/\" + 'DF_RiesgoHip.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt-L0LJpRUJf"
      },
      "outputs": [],
      "source": [
        "# reading the  already \"curated\" data (from google drive)\n",
        "df3 = pd.read_csv(archivo_final ,sep=\";\", low_memory=False)\n",
        "df3['analisis'] = df3['analisis'].astype(str)\n",
        "print(df3.shape)\n",
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM1nPhtuUi2N"
      },
      "source": [
        "# Identification \"models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcfKdNavW0YU"
      },
      "source": [
        "## 1. Hipoglycemia - (model for identifying hypoglycemic events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNhv5PHKUr3g"
      },
      "outputs": [],
      "source": [
        "# read spanish text (analisis: physician's notes) for NLP\n",
        "df_hipo = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_hipo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of unique patients\n",
        "# df_hipo.KeyAnonimo.value_counts()\n",
        "df_hipo.KeyAnonimo.nunique()"
      ],
      "metadata": {
        "id": "7crAQSeOUH7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGYz0GlHWri3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_hipo['Indice_palabra'] = ''\n",
        "df_hipo['Frase_extraida'] = ''\n",
        "palabra1 = \"hipoglicemia\"\n",
        "palabra2 = \"hipoglucemia\"\n",
        "# added: 31/05/2023, based on meeting with Doc:\n",
        "palabra3 = \"baja azucar\"\n",
        "palabra4 = \"azucar bajo\"\n",
        "palabra5 = \"azucar con tendencia a la baja\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoFDXhqmXAYX"
      },
      "outputs": [],
      "source": [
        "arraym = df_hipo['analisis']\n",
        "arraym\n",
        "#type(arraym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ2e4RdCXELI"
      },
      "outputs": [],
      "source": [
        "# Search for the words and generate a new column \"Indice_palabra\" with the index (\"start\") for each record where the search word has been found\n",
        "# Note: find returns an integer representing the index of where the search item was found. If it isn't found, it returns -1.\n",
        "# for i in range(0, df_hipo.shape[0]):\n",
        "#   if (arraym[i].find(palabra1)) != -1:\n",
        "#     indice = arraym[i].find(palabra1)\n",
        "#     df_hipo.Indice_palabra[i] = indice\n",
        "#   else:\n",
        "#     indice = arraym[i].find(palabra2)\n",
        "#     df_hipo.Indice_palabra[i] = indice\n",
        "\n",
        "# with 5 word for the keyword search (31/05/2023):\n",
        "for i in range(0, df_hipo.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_hipo.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_hipo.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_hipo.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_hipo.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_hipo.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2kA30p6uct7"
      },
      "outputs": [],
      "source": [
        "# show the data with the new column that shows this index\n",
        "df_hipo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnY_m5eNXEN9"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\", which shows 150 places (characters) of text around the index of the word found in the previous search\n",
        "# if the word is not found, then show the text \"sin información\"\n",
        "for i in range(0, df_hipo.shape[0]):\n",
        "    if (((df_hipo.Indice_palabra[i]) != -1) & ((df_hipo.Indice_palabra[i]) > 150)):\n",
        "       df_hipo.Frase_extraida[i] = arraym[i][df_hipo.Indice_palabra[i]-150:df_hipo.Indice_palabra[i]+150]\n",
        "    elif (((df_hipo.Indice_palabra[i]) != -1) & ((df_hipo.Indice_palabra[i]) <= 150)):\n",
        "       df_hipo.Frase_extraida[i] = arraym[i][df_hipo.Indice_palabra[i]-df_hipo.Indice_palabra[i]:df_hipo.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_hipo.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_hipo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search for records containing search phrase 3,4 or 5.\n",
        "df_hipo[df_hipo.analisis.str.contains('baja azucar | azucar bajo | azucar con tendencia a la baja')].shape[0]\n",
        "# baja azucar: 1 registro\n",
        "# azucar bajo: 8 registros\n",
        "# azucar con tendencia a la baja: 0 registros\n",
        "#df_hipo[df_hipo.analisis.str.contains('baja azucar | azucar bajo | azucar con tendencia a la baja')].to_excel(\"RH_hipo_conNuevasPalabr.xlsx\")"
      ],
      "metadata": {
        "id": "gxIAlhU9e646"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vxqd48wYAAL"
      },
      "outputs": [],
      "source": [
        "# we download a sample of the data for the manual labeling by practicioning physicians\n",
        "# data_train_hipo = df_hipo['Frase_extraida'].sample(n=20000, random_state=7)\n",
        "# a sample of 20,000 records, regardless of whether there is text in the \"Extracted_phrase\" column or not (\"No information\")\n",
        "# data_train_hipo.to_excel(\"RH_hipo.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhOL6BtzHmLH"
      },
      "outputs": [],
      "source": [
        "# ML MODEL\n",
        "#df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/1. RH_hipo_Ni_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "\n",
        "# read tagged (labeled) data (from gdrive)\n",
        "ruta2 = 'gdrive/My Drive/Colab Notebooks/diabetes_codigoCristhianPatino/Data_RiesgoHipoglicemia/Oraciones para entrenamiento'\n",
        "archivo_RH_hipo_Ni = ruta2 + \"/\" + '1. RH_hipo_Ni_CSV.csv'\n",
        "df_etiquetas = pd.read_csv(archivo_RH_hipo_Ni ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2RSjhz9HmNu"
      },
      "outputs": [],
      "source": [
        "# count labels (classes)\n",
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBTQC6nSHmUO"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js_TApS4I9PR"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq9pba3FI9VE"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta'] # todos los 393 registros\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov9IommKI9X6"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_x_vector.shape"
      ],
      "metadata": {
        "id": "l2U3kesh5hFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVySByHAKz1-"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5L0Kqu6IEgF"
      },
      "outputs": [],
      "source": [
        "# ...to choose the best model (best score) with its parameters\n",
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YnL8dxrKz4g"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000, activation= 'tanh', hidden_layer_sizes= 50, solver= 'lbfgs')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO7uSFvoKz7E"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHafdV58OL-o"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a3hzWHMOMBF"
      },
      "outputs": [],
      "source": [
        "# show the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRZG77ttRUJu"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data. That is, train + test =  total_x_vector.\n",
        "modelo = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000, activation= 'tanh', hidden_layer_sizes= 50, solver= 'lbfgs')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tshFhLj-RUJu"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "# add a 'Hip_Model' column where the label of this columns is \"predice\", based on what has been learned we fill the Hip_Model column in df_hipo.Hip_Model.\n",
        "df_hipo['Modelo_Hip'] = ''\n",
        "\n",
        "for i in range(0, df_hipo.shape[0]):\n",
        "    if ((df_hipo.Indice_palabra[i]) == -1):\n",
        "        df_hipo.Modelo_Hip[i] = 2\n",
        "    else:\n",
        "        palabra = df_hipo.Frase_extraida[i]\n",
        "        df_hipo.Modelo_Hip[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_hipo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to count how many values of \"2\", \"0\" and \"1\" there are. Where label of \"2\", if a patient’s text does not include the search term or key word, the index or label \"0\",\n",
        "# if at least one of these words is present but not directly related to a positive case of the symptom or occurrence of hypoglycemia. Finally,\n",
        "# the index or label \"1\" is applied if the word is present and is directly related to a hypoglycemia / symptoms\n",
        "df_hipo.Modelo_Hip.value_counts()"
      ],
      "metadata": {
        "id": "AwLehWvd9rK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HORfOAahRUJv"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_hipo.to_csv('Modelo_Hip.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0zDaip1g-9Y"
      },
      "source": [
        "## 2. Identification of symptoms of hypoglycemia  - \"cognitive symptom\" model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5rXAvWLfo96"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "df_cog = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CUya7arhKIT"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary)\n",
        "df_cog['Indice_palabra'] = ''\n",
        "df_cog['Frase_extraida'] = ''\n",
        "palabra1 = \"cognitiv\"\n",
        "palabra2 = \"desmayo\"\n",
        "palabra3 = \"problemas de concentracion\"\n",
        "palabra4 = \"desorient\"\n",
        "palabra5 = \"uncion cognitiva\"\n",
        "palabra6 = \"alteracion de la atencion\"\n",
        "palabra7 = \"deterioro de la concentracion\"\n",
        "palabra8 = \"concentrarse\"\n",
        "palabra9 = \" conciencia\"\n",
        "palabra10 = \"confus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AV6oL9Vilpm"
      },
      "outputs": [],
      "source": [
        "arraym = df_cog['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJEkqgkHil4h"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_cog.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra5)) != -1:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra6)) != -1:\n",
        "    indice = arraym[i].find(palabra6)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra7)) != -1:\n",
        "    indice = arraym[i].find(palabra7)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra8)) != -1:\n",
        "    indice = arraym[i].find(palabra8)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra9)) != -1:\n",
        "    indice = arraym[i].find(palabra9)\n",
        "    df_cog.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra10)\n",
        "    df_cog.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clNydlU1jLSL"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 places (characters) of text around the index of the word found in the previous search\n",
        "for i in range(0, df_cog.shape[0]):\n",
        "    if (((df_cog.Indice_palabra[i]) != -1) & ((df_cog.Indice_palabra[i]) > 150)):\n",
        "       df_cog.Frase_extraida[i] = arraym[i][df_cog.Indice_palabra[i]-150:df_cog.Indice_palabra[i]+150]\n",
        "    elif (((df_cog.Indice_palabra[i]) != -1) & ((df_cog.Indice_palabra[i]) <= 150)):\n",
        "       df_cog.Frase_extraida[i] = arraym[i][df_cog.Indice_palabra[i]-df_cog.Indice_palabra[i]:df_cog.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_cog.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVtI0108jLWb"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_cog = df_cog['Frase_extraida'].sample(n=30000, random_state=7)\n",
        "#data_train_cog.to_excel(\"RH_cog.xlsx\")\n",
        "#data_train_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24AsREVMRUJy"
      },
      "outputs": [],
      "source": [
        "# ML MODEL\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/2. RH_cog_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F47F8cBcRUJy"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u4PisZzRUJz"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw0Aeoc-RUJz"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zuWyAN4RUJz"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfqM8yR7RUJ0"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3S84cjXRUJ0"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dAQQuzcRUJ1"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKuQolJaRUJ2"
      },
      "outputs": [],
      "source": [
        "# use the best model\n",
        "modelo = SVC(random_state=11, C = 10, gamma = 0.1, kernel = 'linear')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2yBnPABRUJ2"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE989diuRUJ3"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXHpp36zRUJ3"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_6b-YRXRUJ4"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = SVC(random_state=11, C = 10, gamma = 0.1, kernel = 'linear')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNlvWVh6RUJ4"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_cog['Modelo_Cog'] = ''\n",
        "\n",
        "for i in range(0, df_cog.shape[0]):\n",
        "    if ((df_cog.Indice_palabra[i]) == -1):\n",
        "        df_cog.Modelo_Cog[i] = 2\n",
        "    else:\n",
        "        palabra = df_cog.Frase_extraida[i]\n",
        "        df_cog.Modelo_Cog[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkuPMqasRUJ5"
      },
      "outputs": [],
      "source": [
        "# save DF as csv\n",
        "df_cog.to_csv('Modelo_Cog.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvbj_hH3lUDL"
      },
      "source": [
        "## Identifcation of the \"tremor symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYz9fLNklTTj"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_temb = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_temb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBJCEJJUlTbR"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_temb['Indice_palabra'] = ''\n",
        "df_temb['Frase_extraida'] = ''\n",
        "palabra1 = \"parestesia\"\n",
        "palabra2 = \"temblor\"\n",
        "palabra3 = \"hormigueo\"\n",
        "palabra4 = \"sacudida\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUZp7zIrl4Cp"
      },
      "outputs": [],
      "source": [
        "arraym = df_temb['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S7yPC59l8VD"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_temb.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_temb.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_temb.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_temb.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_temb.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFtdXwJIl8Xt"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_temb.shape[0]):\n",
        "    if (((df_temb.Indice_palabra[i]) != -1) & ((df_temb.Indice_palabra[i]) > 150)):\n",
        "       df_temb.Frase_extraida[i] = arraym[i][df_temb.Indice_palabra[i]-150:df_temb.Indice_palabra[i]+150]\n",
        "    elif (((df_temb.Indice_palabra[i]) != -1) & ((df_temb.Indice_palabra[i]) <= 150)):\n",
        "       df_temb.Frase_extraida[i] = arraym[i][df_temb.Indice_palabra[i]-df_temb.Indice_palabra[i]:df_temb.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_temb.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_temb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMQb-NGEl8Zq"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_temb = df_temb['Frase_extraida'].sample(n=30000, random_state=7)\n",
        "#data_train_temb.to_excel(\"RH_temb.xlsx\")\n",
        "#data_train_temb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kj9UDHjRUJ9"
      },
      "outputs": [],
      "source": [
        "# ML model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/3. RH_temb_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boYtwqkoRUJ9"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qKDWykjRUJ-"
      },
      "outputs": [],
      "source": [
        "#  graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbtlJ5I2RUJ-"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b1xQrsGRUJ-"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStlBBOVRUJ-"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm5YJfm7RUJ_"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-8WdoSRRUJ_"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir83iZFQRUJ_"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = SVC(random_state=11, C = 1, gamma = 0.1, kernel = 'linear')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtKc37oMRUKA"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byk3SRJTRUKA"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYXjyxFaRUKA"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkJH0IreRUKA"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = SVC(random_state=11, C = 1, gamma = 0.1, kernel = 'linear')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnUajQHWRUKB"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_temb['Modelo_temb'] = ''\n",
        "\n",
        "for i in range(0, df_temb.shape[0]):\n",
        "    if ((df_temb.Indice_palabra[i]) == -1):\n",
        "        df_temb.Modelo_temb[i] = 2\n",
        "    else:\n",
        "        palabra = df_temb.Frase_extraida[i]\n",
        "        df_temb.Modelo_temb[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_temb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bizNCx0aRUKB"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_temb.to_csv('Modelo_temb.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-vVp4nIcW-w"
      },
      "source": [
        "## Identification of the \"cardiac symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otRydNpzcWmd"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_card = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tNvhJe4cljb"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_card['Indice_palabra'] = ''\n",
        "df_card['Frase_extraida'] = ''\n",
        "palabra1 = \"frecuencia cardiac\"\n",
        "palabra2 = \"taquicardia\"\n",
        "palabra3 = \"palpit\"\n",
        "palabra4 = \"latido\"\n",
        "palabra5 = \"latidos cardiaco\"\n",
        "palabra6 = \"ritmo\"\n",
        "palabra7 = \" fc \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDOprsGDclmH"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_card.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra5)) != -1:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra6)) != -1:\n",
        "    indice = arraym[i].find(palabra6)\n",
        "    df_card.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra7)\n",
        "    df_card.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmMmHN5Qclol"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_card.shape[0]):\n",
        "    if (((df_card.Indice_palabra[i]) != -1) & ((df_card.Indice_palabra[i]) > 150)):\n",
        "       df_card.Frase_extraida[i] = arraym[i][df_card.Indice_palabra[i]-150:df_card.Indice_palabra[i]+150]\n",
        "    elif (((df_card.Indice_palabra[i]) != -1) & ((df_card.Indice_palabra[i]) <= 150)):\n",
        "       df_card.Frase_extraida[i] = arraym[i][df_card.Indice_palabra[i]-df_card.Indice_palabra[i]:df_card.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_card.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJja2GNufo9A"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_card = df_card['Frase_extraida'].sample(n=20000, random_state=7)\n",
        "#data_train_card.to_excel(\"RH_card.xlsx\")\n",
        "#data_train_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFd4-GKIRUKD"
      },
      "outputs": [],
      "source": [
        "# ML Model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/4. RH_card_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-_K6yfHRUKD"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBwxP8eERUKF"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzJfv7sARUKF"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsSVCrvxRUKF"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWaJO2zCRUKF"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaSfD1XVRUKG"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oZDgaLARUKG"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iIWu_ZYRUKG"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = DecisionTreeClassifier(random_state=11, criterion = 'gini')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5nq__WQRUKH"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-V_SZnJRUKH"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lid9BQsjRUKH"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxDXApgrRUKH"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = DecisionTreeClassifier(random_state=11, criterion = 'gini')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRJHgHX5RUKH"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_card['Modelo_card'] = ''\n",
        "\n",
        "for i in range(0, df_card.shape[0]):\n",
        "    if ((df_card.Indice_palabra[i]) == -1):\n",
        "        df_card.Modelo_card[i] = 2\n",
        "    else:\n",
        "        palabra = df_card.Frase_extraida[i]\n",
        "        df_card.Modelo_card[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcTmrdpARUKH"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_card.to_csv('Modelo_card.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBSFvQfVnwtC"
      },
      "source": [
        "## Identification of the \"vision symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZSEdNZnw3Z"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_vis = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVMSC6onooqn"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_vis['Indice_palabra'] = ''\n",
        "df_vis['Frase_extraida'] = ''\n",
        "palabra1 = \"vision borros\"\n",
        "palabra2 = \" vision\"\n",
        "palabra3 = \" visual\"\n",
        "palabra4 = \"enfocar\"\n",
        "palabra5 = \"perdida visual\"\n",
        "palabra6 = \" ver \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2NOPR92ootc"
      },
      "outputs": [],
      "source": [
        "arraym = df_vis['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7siOLyVKpHYx"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_vis.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_vis.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_vis.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_vis.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_vis.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra5)) != -1:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_vis.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra6)\n",
        "    df_vis.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZMnh94gpHba"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_vis.shape[0]):\n",
        "    if (((df_vis.Indice_palabra[i]) != -1) & ((df_vis.Indice_palabra[i]) > 150)):\n",
        "       df_vis.Frase_extraida[i] = arraym[i][df_vis.Indice_palabra[i]-150:df_vis.Indice_palabra[i]+150]\n",
        "    elif (((df_vis.Indice_palabra[i]) != -1) & ((df_vis.Indice_palabra[i]) <= 150)):\n",
        "       df_vis.Frase_extraida[i] = arraym[i][df_vis.Indice_palabra[i]-df_vis.Indice_palabra[i]:df_vis.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_vis.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNcb6afypHeI"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_vis = df_vis['Frase_extraida'].sample(n=20000, random_state=7)\n",
        "#data_train_vis.to_excel(\"RH_vis.xlsx\")\n",
        "#data_train_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMjQz3McRUKJ"
      },
      "outputs": [],
      "source": [
        "# ML Model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/5. RH_vis_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uDXbLlqRUKJ"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LlvzMf4RUKJ"
      },
      "outputs": [],
      "source": [
        "#  graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_pNYsnmRUKJ"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GBs-J8KRUKK"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BW7vwBNRUKK"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRzLFFzJRUKK"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3UcVkPYRUKK"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEUFZLYlRUKL"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = SVC(random_state=11, C = 1, gamma = 0.1, kernel = 'linear' )\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ3_Eu7ORUKL"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq9lBVnqRUKL"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vV6duPIRUKL"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uSlORChRUKL"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = SVC(random_state=11, C = 1, gamma = 0.1, kernel = 'linear' )\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWj1UD9NRUKM"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_vis['Modelo_vis'] = ''\n",
        "\n",
        "for i in range(0, df_vis.shape[0]):\n",
        "    if ((df_vis.Indice_palabra[i]) == -1):\n",
        "        df_vis.Modelo_vis[i] = 2\n",
        "    else:\n",
        "        palabra = df_vis.Frase_extraida[i]\n",
        "        df_vis.Modelo_vis[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_tv8rF_RUKM"
      },
      "outputs": [],
      "source": [
        "# Save DF to csv\n",
        "df_vis.to_csv('Modelo_vis.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvMw5UdmwCsh"
      },
      "source": [
        "## Identification of the \"irritability symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfRYMxkcwCMA"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_irri = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_irri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoD8bkzAwCO2"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_irri['Indice_palabra'] = ''\n",
        "df_irri['Frase_extraida'] = ''\n",
        "palabra1 = \"frenetic\"\n",
        "palabra2 = \"ansios\"\n",
        "palabra3 = \"ansiedad\"\n",
        "palabra4 = \"angustia\"\n",
        "palabra5 = \"nervios\"\n",
        "palabra6 = \"depresi\"\n",
        "palabra7 = \"locura\"\n",
        "palabra8 = \"triste\" #irritable\n",
        "palabra9 = \"estres\" #alterad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx9mmSccwucj"
      },
      "outputs": [],
      "source": [
        "arraym = df_irri['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDVTRxIpwufa"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_irri.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra5)) != -1:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra6)) != -1:\n",
        "    indice = arraym[i].find(palabra6)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra7)) != -1:\n",
        "    indice = arraym[i].find(palabra7)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra8)) != -1:\n",
        "    indice = arraym[i].find(palabra8)\n",
        "    df_irri.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra9)\n",
        "    df_irri.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1MlJ-ZawuiW"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_irri.shape[0]):\n",
        "    if (((df_irri.Indice_palabra[i]) != -1) & ((df_irri.Indice_palabra[i]) > 150)):\n",
        "       df_irri.Frase_extraida[i] = arraym[i][df_irri.Indice_palabra[i]-150:df_irri.Indice_palabra[i]+150]\n",
        "    elif (((df_irri.Indice_palabra[i]) != -1) & ((df_irri.Indice_palabra[i]) <= 150)):\n",
        "       df_irri.Frase_extraida[i] = arraym[i][df_irri.Indice_palabra[i]-df_irri.Indice_palabra[i]:df_irri.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_irri.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_irri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RveWr01J1U7k"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_irri = df_irri['Frase_extraida'].sample(n=30000, random_state=7)\n",
        "#data_train_irri.to_excel(\"RH_irri.xlsx\")\n",
        "#data_train_irri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUX2bc8JRUKN"
      },
      "outputs": [],
      "source": [
        "# ML Model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/6. RH_irri_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIE-BcVJRUKO"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkApsg1-RUKO"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTpBgIZXRUKO"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkfwLUzCRUKP"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgGIFNVPRUKP"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztp6BMXcRUKP"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWwNaw2uRUKQ"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-Ew7IzRUKQ"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = SVC(random_state=11, C = 10, gamma = 0.1, kernel = 'rbf')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uugp4aPJRUKR"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxfOKd_ZRUKR"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJMlBc_RUKR"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eY3ry8vRUKS"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = SVC(random_state=11, C = 10, gamma = 0.1, kernel = 'rbf')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fCQF2jRUKS"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_irri['Modelo_irri'] = ''\n",
        "\n",
        "for i in range(0, df_irri.shape[0]):\n",
        "    if ((df_irri.Indice_palabra[i]) == -1):\n",
        "        df_irri.Modelo_irri[i] = 2\n",
        "    else:\n",
        "        palabra = df_irri.Frase_extraida[i]\n",
        "        df_irri.Modelo_irri[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_irri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpOgQWCRUKS"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_irri.to_csv('Modelo_irri.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puFvdwe68foM"
      },
      "source": [
        "## Identification of the \"sweating symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22ivtnAq8fvB"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_sud = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_sud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhisoEww8fxd"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_sud['Indice_palabra'] = ''\n",
        "df_sud['Frase_extraida'] = ''\n",
        "palabra1 = \"diaforesis\"\n",
        "palabra2 = \"hiperhidrosis\"\n",
        "palabra3 = \"sudor\"\n",
        "palabra4 = \"transpir\"\n",
        "palabra5 = \"suda \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66ivu1IX9aOP"
      },
      "outputs": [],
      "source": [
        "arraym = df_sud['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "591HmNsE9k-2"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_sud.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_sud.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_sud.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra3)) != -1:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_sud.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra4)) != -1:\n",
        "    indice = arraym[i].find(palabra4)\n",
        "    df_sud.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra5)\n",
        "    df_sud.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhKOoXqU9lB3"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_sud.shape[0]):\n",
        "    if (((df_sud.Indice_palabra[i]) != -1) & ((df_sud.Indice_palabra[i]) > 150)):\n",
        "       df_sud.Frase_extraida[i] = arraym[i][df_sud.Indice_palabra[i]-150:df_sud.Indice_palabra[i]+150]\n",
        "    elif (((df_sud.Indice_palabra[i]) != -1) & ((df_sud.Indice_palabra[i]) <= 150)):\n",
        "       df_sud.Frase_extraida[i] = arraym[i][df_sud.Indice_palabra[i]-df_sud.Indice_palabra[i]:df_sud.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_sud.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_sud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jQsk80mAFNY"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_sud = df_sud['Frase_extraida'].sample(n=60000, random_state=7)\n",
        "#data_train_sud.to_excel(\"RH_sud.xlsx\")\n",
        "#data_train_sud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiEobjy5RUKV"
      },
      "outputs": [],
      "source": [
        "# ML Model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/7. RH_sud_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWkIHMlJRUKX"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPv3VJjnRUKY"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6HWWk_xRUKZ"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSF5wiRmRUKZ"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC4JlgcBRUKa"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_ME0br3RUKa"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfjgoTq2RUKa"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjBg4v0QRUKb",
        "outputId": "e226cb04-d333-4ec8-9e4a-55850526d221"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier(metric='manhattan', n_jobs=-1, n_neighbors=1)"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# using the best model\n",
        "modelo = KNeighborsClassifier(n_jobs = -1, metric = 'manhattan', n_neighbors = 1)\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oSveo-JRUKb"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdxHv84GRUKb"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yB3X7npRUKb"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylrGs2sMRUKc"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = KNeighborsClassifier(n_jobs = -1, metric = 'manhattan', n_neighbors = 1)\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y_Pe159RUKc"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_sud['Modelo_sud'] = ''\n",
        "\n",
        "for i in range(0, df_sud.shape[0]):\n",
        "    if ((df_sud.Indice_palabra[i]) == -1):\n",
        "        df_sud.Modelo_sud[i] = 2\n",
        "    else:\n",
        "        palabra = df_sud.Frase_extraida[i]\n",
        "        df_sud.Modelo_sud[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_sud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GM3KZuRRUKc"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_sud.to_csv('Modelo_sud.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhPTESrHBYiq"
      },
      "source": [
        "## Identification of the \"discomfort symptom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sxTLqQ5BYpV"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "df_vom = df3[[\"KeyAnonimo\", 'analisis']]\n",
        "df_vom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8nNzTf2Bl7b"
      },
      "outputs": [],
      "source": [
        "# search terms - key words (list of words - \"dictionary\")\n",
        "df_vom['Indice_palabra'] = ''\n",
        "df_vom['Frase_extraida'] = ''\n",
        "palabra1 = \"nausea\"\n",
        "palabra2 = \"vomit\"\n",
        "palabra3 = \"diarrea\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZwcctXxBl-O"
      },
      "outputs": [],
      "source": [
        "arraym = df_vom['analisis']\n",
        "arraym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCYtPf8qBmAu"
      },
      "outputs": [],
      "source": [
        "# performing the search...\n",
        "for i in range(0, df_vom.shape[0]):\n",
        "  if (arraym[i].find(palabra1)) != -1:\n",
        "    indice = arraym[i].find(palabra1)\n",
        "    df_vom.Indice_palabra[i] = indice\n",
        "  elif (arraym[i].find(palabra2)) != -1:\n",
        "    indice = arraym[i].find(palabra2)\n",
        "    df_vom.Indice_palabra[i] = indice\n",
        "  else:\n",
        "    indice = arraym[i].find(palabra3)\n",
        "    df_vom.Indice_palabra[i] = indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V8WikLLB8Uz"
      },
      "outputs": [],
      "source": [
        "# create a new column \"Frase_extraida\" that displays 150 characters of text around the index of the word found in the previous search.\n",
        "for i in range(0, df_vom.shape[0]):\n",
        "    if (((df_vom.Indice_palabra[i]) != -1) & ((df_vom.Indice_palabra[i]) > 150)):\n",
        "       df_vom.Frase_extraida[i] = arraym[i][df_vom.Indice_palabra[i]-150:df_vom.Indice_palabra[i]+150]\n",
        "    elif (((df_vom.Indice_palabra[i]) != -1) & ((df_vom.Indice_palabra[i]) <= 150)):\n",
        "       df_vom.Frase_extraida[i] = arraym[i][df_vom.Indice_palabra[i]-df_vom.Indice_palabra[i]:df_vom.Indice_palabra[i]+150]\n",
        "    else:\n",
        "       df_vom.Frase_extraida[i] = \"Sin información\"\n",
        "\n",
        "df_vom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkX3_EbxB8XP"
      },
      "outputs": [],
      "source": [
        "# use a sample of the data for manual labeling by practicing physicians\n",
        "#data_train_vom = df_vom['Frase_extraida'].sample(n=60000, random_state=7)\n",
        "#data_train_vom.to_excel(\"RH_vom.xlsx\")\n",
        "#data_train_vom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBCFtXoHRUKd"
      },
      "outputs": [],
      "source": [
        "# ML Model\n",
        "# read the labeled data\n",
        "df_etiquetas = pd.read_csv('C:/Users/diabetes.ml1/Downloads/Data_RiesgoHipoglicemia/Oraciones para etiquetar/8. RH_vom_CP_CSV.csv' ,sep=\";\", low_memory=False)\n",
        "df_etiquetas = df_etiquetas[['Frase_extraida', 'Etiqueta']]\n",
        "df_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoyOm8OYRUKd"
      },
      "outputs": [],
      "source": [
        "df_etiquetas.value_counts('Etiqueta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX1T6A4xRUKd"
      },
      "outputs": [],
      "source": [
        "# graph the proportion of classes as a pie chart\n",
        "plt.pie(df_etiquetas['Etiqueta'].value_counts(),\n",
        "        labels = ['0','1'], autopct='%.2f%%');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njieou6VRUKe"
      },
      "outputs": [],
      "source": [
        "df = df_etiquetas.values\n",
        "Y = df[:,1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm3VAkeuRUKe"
      },
      "outputs": [],
      "source": [
        "# create training and test partition: 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_etiquetas, test_size = 0.2, random_state = 10, stratify = df_etiquetas['Etiqueta'])\n",
        "\n",
        "X_train, y_train = train['Frase_extraida'], train['Etiqueta']\n",
        "X_test, y_test = test['Frase_extraida'], test['Etiqueta']\n",
        "X_total, y_total = df_etiquetas['Frase_extraida'], df_etiquetas['Etiqueta']\n",
        "\n",
        "\n",
        "print(np.unique(Y,return_counts=True))\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(np.unique(y_total,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHY5RisiRUKe"
      },
      "outputs": [],
      "source": [
        "# vectorization to convert words to numbers for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "total_x_vector = tfidf.fit_transform(X_total)\n",
        "train_x_vector = tfidf.transform(X_train)\n",
        "test_x_vector = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CagYG18LRUKe"
      },
      "outputs": [],
      "source": [
        "# run the different classification models, using grid search to find the optimal hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#DecisionTree\n",
        "dt_param_grid = {'criterion':['gini','entropy']}\n",
        "dt = DecisionTreeClassifier(random_state=11)\n",
        "dt_cv = GridSearchCV(dt, dt_param_grid, cv=10)\n",
        "dt_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#SVM\n",
        "sv_param_grid = {'kernel':['rbf','linear', 'poly'],\n",
        "              'C': [0.5,1,10,50,100],\n",
        "              'gamma':[0.1, 'auto']}\n",
        "sv = SVC(random_state=11)\n",
        "sv_cv = GridSearchCV(sv , sv_param_grid, cv=10)\n",
        "sv_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#G_NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb_accuracy_train = (cross_val_score(nb, train_x_vector.toarray(), y_train, cv=10)).mean()\n",
        "\n",
        "#Regresión logistica\n",
        "lg_param_grid = {'C':[1,10,100],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'max_iter':[7000]}\n",
        "lg = LogisticRegression(random_state=11)\n",
        "lg_cv= GridSearchCV(lg, lg_param_grid, cv=10)\n",
        "lg_cv.fit(train_x_vector, y_train)\n",
        "\n",
        "#KNN\n",
        "knn_param_grid = {'n_neighbors':np.arange(1,21),\n",
        "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
        "knn = KNeighborsClassifier(n_jobs = -1)\n",
        "knn_cv= GridSearchCV(knn, knn_param_grid,cv=10)\n",
        "knn_cv.fit(train_x_vector,y_train)\n",
        "\n",
        "#MLP\n",
        "#mlp_param_grid = {'hidden_layer_sizes':[10, 50],\n",
        "#              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
        "#              'solver':['lbfgs', 'sgd', 'adam']}\n",
        "#mlp = MLPClassifier(alpha=1e-5, random_state = 11, max_iter=2000)\n",
        "#mlp_cv = GridSearchCV(mlp , mlp_param_grid, cv=10)\n",
        "#mlp_cv.fit(train_x_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN_fC5Z6RUKf"
      },
      "outputs": [],
      "source": [
        "print(f'DT: {dt_cv.best_score_}, {dt_cv.best_params_}')\n",
        "print(f'SVM: {sv_cv.best_score_}, {sv_cv.best_params_}')\n",
        "print(f'GNB: {nb_accuracy_train}')\n",
        "print(f'LG: {lg_cv.best_score_}, {lg_cv.best_params_}')\n",
        "print(f'KNN: {knn_cv.best_score_}, {knn_cv.best_params_}')\n",
        "#print(f'MLP: {mlp_cv.best_score_}, {mlp_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YIJnQCORUKf"
      },
      "outputs": [],
      "source": [
        "# using the best model\n",
        "modelo = LogisticRegression(random_state=11, C = 100, max_iter = 7000, solver = 'newton-cg')\n",
        "modelo.fit(train_x_vector,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgulpZLRUKf"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpvD-EMLRUKf"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix\n",
        "cm=confusion_matrix(y_test, modelo.predict(test_x_vector))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grxZbVgKRUKf"
      },
      "outputs": [],
      "source": [
        "# display the main performance indicators: precision, recall, f1-score, support, accuracy\n",
        "print(classification_report(y_test, modelo.predict(test_x_vector),\n",
        "                      labels = classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzgU4LP_RUKf"
      },
      "outputs": [],
      "source": [
        "# train with 100% of the data\n",
        "modelo = LogisticRegression(random_state=11, C = 100, max_iter = 7000, solver = 'newton-cg')\n",
        "modelo.fit(total_x_vector,y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvtKsOgpRUKg"
      },
      "outputs": [],
      "source": [
        "# label the rest of the dataset (performing the predictions)\n",
        "df_vom['Modelo_vom'] = ''\n",
        "\n",
        "for i in range(0, df_vom.shape[0]):\n",
        "    if ((df_vom.Indice_palabra[i]) == -1):\n",
        "        df_vom.Modelo_vom[i] = 2\n",
        "    else:\n",
        "        palabra = df_vom.Frase_extraida[i]\n",
        "        df_vom.Modelo_vom[i] = modelo.predict(tfidf.transform([palabra]))\n",
        "\n",
        "df_vom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXbLQRwERUKg"
      },
      "outputs": [],
      "source": [
        "# save DF to csv\n",
        "df_vom.to_csv('Modelo_vom.csv', sep=';')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}